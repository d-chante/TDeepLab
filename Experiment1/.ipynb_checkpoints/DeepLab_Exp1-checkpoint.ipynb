{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff70fb9e",
   "metadata": {},
   "source": [
    "# DeepLab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52240bd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-19 00:44:46.100125: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Oct 19 00:44:48 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.125.06   Driver Version: 525.125.06   CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla P40           Off  | 00000000:05:00.0 Off |                  Off |\n",
      "| N/A   43C    P8    16W / 250W |      0MiB / 24576MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "IMPORTS\n",
    "'''\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv2D, BatchNormalization, Dropout, Activation, UpSampling2D\n",
    "from tensorflow.keras.layers import AveragePooling2D, Conv2DTranspose, Concatenate, Input, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.applications import ResNet101\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.metrics import BinaryAccuracy\n",
    "from tensorflow.image import resize, ResizeMethod\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "from numpy import load\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "'''\n",
    "DATA PATHS\n",
    "'''\n",
    "TOP_DIR = '/tf/Notebooks/Iwashita'\n",
    "\n",
    "TRAIN_DIR = TOP_DIR + '/Data/Preprocessed_wAugmentation/Experiment1/Train'\n",
    "VAL_DIR = TOP_DIR + '/Data/Preprocessed_wAugmentation/Experiment1/Validate'\n",
    "TEST_DIR = TOP_DIR + '/Data/Preprocessed_wAugmentation/Experiment1/Test'\n",
    "\n",
    "'''\n",
    "OUTPUTS PATH\n",
    "'''\n",
    "WEIGHTS_PATH = TOP_DIR + '/Output/Weights/'\n",
    "METRICS_PATH = TOP_DIR + '/Output/Metrics/'\n",
    "\n",
    "'''\n",
    "GPU\n",
    "'''\n",
    "gpu_p40 = '/device:GPU:0'\n",
    "\n",
    "get_ipython().system('nvidia-smi')\n",
    "\n",
    "'''\n",
    "TRAINING DATA\n",
    "'''\n",
    "exp1_rgb_X_train = load(TRAIN_DIR + '/exp1_rgb_X_train_dl.npy')\n",
    "exp1_y_train = load(TRAIN_DIR + '/exp1_y_train_dl.npy')\n",
    "\n",
    "'''\n",
    "VALIDATION DATA\n",
    "'''\n",
    "exp1_rgb_X_val = load(VAL_DIR + '/exp1_rgb_X_val_dl.npy')\n",
    "exp1_y_val = load(VAL_DIR + '/exp1_y_val_dl.npy')\n",
    "\n",
    "'''\n",
    "TEST DATA\n",
    "'''\n",
    "exp1_rgb_X_test = load(TEST_DIR + '/exp1_rgb_X_test_dl.npy')\n",
    "exp1_y_test = load(TEST_DIR + '/exp1_y_test_dl.npy')\n",
    "\n",
    "'''\n",
    "INTERSECTION OVER UNION\n",
    "'''\n",
    "def iou(y_true, y_pred, num_classes):\n",
    "    intersection = np.histogram2d(y_true.flatten(), y_pred.flatten(), bins=num_classes)[0]\n",
    "    area_true = np.histogram(y_true, bins=num_classes)[0]\n",
    "    area_pred = np.histogram(y_pred, bins=num_classes)[0]\n",
    "    area_true = np.expand_dims(area_true, -1)\n",
    "    area_pred = np.expand_dims(area_pred, 0)\n",
    "\n",
    "    union = area_true + area_pred - intersection\n",
    "\n",
    "    union[union == 0] = 1e-9\n",
    "    iou = intersection / union\n",
    "\n",
    "    return iou, np.mean(np.diag(iou))\n",
    "\n",
    "'''\n",
    "PIXEL ACCURACY\n",
    "'''\n",
    "def pixel_accuracy(y_true, y_pred):\n",
    "    return np.sum(y_true == y_pred) / y_true.size\n",
    "\n",
    "'''\n",
    "MEAN ACCURACY\n",
    "'''\n",
    "def mean_accuracy(y_true, y_pred, num_classes):\n",
    "    intersection = np.histogram2d(y_true.flatten(), y_pred.flatten(), bins=num_classes)[0]\n",
    "    area_true = np.histogram(y_true, bins=num_classes)[0]\n",
    "\n",
    "    area_true[area_true == 0] = 1e-9\n",
    "    accuracy = np.diag(intersection) / area_true\n",
    "\n",
    "    return np.mean(accuracy)\n",
    "\n",
    "'''\n",
    "FREQUENCY-WEIGHTED INTERSECTION OVER UNION\n",
    "'''\n",
    "def fw_iou(y_true, y_pred, num_classes):\n",
    "    intersection = np.histogram2d(y_true.flatten(), y_pred.flatten(), bins=num_classes)[0]\n",
    "    area_true = np.histogram(y_true, bins=num_classes)[0]\n",
    "    area_pred = np.histogram(y_pred, bins=num_classes)[0]\n",
    "    area_true = np.expand_dims(area_true, -1)\n",
    "    area_pred = np.expand_dims(area_pred, 0)\n",
    "\n",
    "    union = area_true + area_pred - intersection\n",
    "\n",
    "    union[union == 0] = 1e-9\n",
    "    iou = intersection / union\n",
    "    fw_iou = np.sum(area_true * iou) / np.sum(area_true)\n",
    "\n",
    "    return fw_iou\n",
    "\n",
    "def display_one_hot_annotation(annotations_onehot):\n",
    "    label = np.argmax(annotations_onehot, axis=-1)\n",
    "    cmap = plt.get_cmap('tab10', 7)\n",
    "\n",
    "    plt.imshow(label, cmap=cmap)\n",
    "    plt.colorbar(ticks=range(7), format=plt.FuncFormatter(lambda val, loc: {\n",
    "        0: \"unlabeled\",\n",
    "        1: \"sand\",\n",
    "        2: \"soil\",\n",
    "        3: \"ballast\",\n",
    "        4: \"rock\",\n",
    "        5: \"bedrock\",\n",
    "        6: \"rocky terrain\"\n",
    "    }[val]))\n",
    "    plt.show()\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc8cda4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(56, 572, 572, 3)\n",
      "(56, 572, 572, 7)\n",
      "(28, 572, 572, 3)\n",
      "(28, 572, 572, 7)\n",
      "(28, 572, 572, 3)\n",
      "(28, 572, 572, 7)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(exp1_rgb_X_train.shape)\n",
    "print(exp1_y_train.shape)\n",
    "\n",
    "print(exp1_rgb_X_val.shape)\n",
    "print(exp1_y_val.shape)\n",
    "\n",
    "print(exp1_rgb_X_test.shape)\n",
    "print(exp1_y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5437c4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "MODEL PARAMS\n",
    "'''\n",
    "INPUT_DIMS = (572, 572, 3)\n",
    "\n",
    "BATCH_SIZE = 2\n",
    "EPOCHS = 1000 \n",
    "\n",
    "LEARNING_RATE = 1e-4\n",
    "PATIENCE = 15\n",
    "FACTOR = 0.1\n",
    "\n",
    "DROPOUT_RATE = 0.5\n",
    "\n",
    "TRAINABLE_LAYERS = 300\n",
    "\n",
    "LOSS_ = BinaryCrossentropy()\n",
    "METRICS_ = [BinaryAccuracy()]\n",
    "\n",
    "EXP1_FILENAME = \"deeplab_baseline_exp1_batch{}_epoch{}_lr{}_p{}_f{}\".format(\n",
    "    BATCH_SIZE, EPOCHS, LEARNING_RATE, PATIENCE, FACTOR)\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d98e7ac",
   "metadata": {},
   "source": [
    "### DeepLab Common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f711169",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Atrous Spatial Pyramid Pooling\n",
    "'''\n",
    "def ASPP(inputs):\n",
    "    shape = inputs.shape\n",
    "\n",
    "    y_pool = AveragePooling2D(pool_size=(shape[1], shape[2]), name='average_pooling')(inputs)\n",
    "    y_pool = Conv2D(filters=256, kernel_size=1, padding='same', use_bias=False)(y_pool)\n",
    "    y_pool = BatchNormalization(name=f'bn_1')(y_pool)\n",
    "    y_pool = Activation('relu', name=f'relu_1')(y_pool)\n",
    "    y_pool = UpSampling2D((shape[1], shape[2]), interpolation=\"bilinear\")(y_pool)\n",
    "\n",
    "    y_1 = Conv2D(filters=256, kernel_size=1, dilation_rate=1, padding='same', use_bias=False)(inputs)\n",
    "    y_1 = BatchNormalization()(y_1)\n",
    "    y_1 = Activation('relu')(y_1)\n",
    "\n",
    "    y_6 = Conv2D(filters=256, kernel_size=3, dilation_rate=6, padding='same', use_bias=False)(inputs)\n",
    "    y_6 = BatchNormalization()(y_6)\n",
    "    y_6 = Activation('relu')(y_6)\n",
    "\n",
    "    y_12 = Conv2D(filters=256, kernel_size=3, dilation_rate=12, padding='same', use_bias=False)(inputs)\n",
    "    y_12 = BatchNormalization()(y_12)\n",
    "    y_12 = Activation('relu')(y_12)\n",
    "\n",
    "    y_18 = Conv2D(filters=256, kernel_size=3, dilation_rate=18, padding='same', use_bias=False)(inputs)\n",
    "    y_18 = BatchNormalization()(y_18)\n",
    "    y_18 = Activation('relu')(y_18)\n",
    "\n",
    "    y = Concatenate()([y_pool, y_1, y_6, y_12, y_18])\n",
    "\n",
    "    y = Conv2D(filters=256, kernel_size=1, dilation_rate=1, padding='same', use_bias=False)(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Activation('relu')(y)\n",
    "    return y\n",
    "\n",
    "def DeepLab(shape):\n",
    "    \"\"\" Inputs \"\"\"\n",
    "    inputs = Input(shape)\n",
    "\n",
    "    \"\"\" Pre-trained ResNet101 \"\"\"\n",
    "    base_model = ResNet101(weights='imagenet', include_top=False, input_tensor=inputs)\n",
    "    \n",
    "    for layer in base_model.layers[:TRAINABLE_LAYERS]:\n",
    "        layer.trainable = False\n",
    "    for layer in base_model.layers[TRAINABLE_LAYERS:]:\n",
    "        layer.trainable = True\n",
    "\n",
    "    \"\"\" Pre-trained ResNet101 Output \"\"\"\n",
    "    image_features = base_model.get_layer('conv4_block23_out').output\n",
    "    x_a = ASPP(image_features)\n",
    "    x_a = UpSampling2D((4, 4), interpolation=\"bilinear\")(x_a)\n",
    "\n",
    "    \"\"\" Get low-level features \"\"\"\n",
    "    x_b = base_model.get_layer('conv2_block2_out').output\n",
    "    x_b = Conv2D(filters=48, kernel_size=1, padding='same', use_bias=False)(x_b)\n",
    "    x_b = BatchNormalization()(x_b)\n",
    "    x_b = Activation('relu')(x_b)\n",
    "\n",
    "    \"\"\" Spatial Pyramid Pooling (SPP) \"\"\"\n",
    "    # Apply different pooling sizes\n",
    "    pool1 = GlobalAveragePooling2D()(x_a)\n",
    "    pool2 = GlobalAveragePooling2D()(x_a)\n",
    "    pool3 = GlobalAveragePooling2D()(x_a)\n",
    "\n",
    "    # Concatenate pooled features\n",
    "    spp_features = Concatenate()([pool1, pool2, pool3])\n",
    "\n",
    "    # Reshape to match x_b's shape\n",
    "    spp_features = tf.keras.layers.Reshape((1, 1, -1))(spp_features)\n",
    "    spp_features = UpSampling2D((x_b.shape[1], x_b.shape[2]), interpolation=\"bilinear\")(spp_features)\n",
    "\n",
    "    x = Concatenate()([spp_features, x_b])\n",
    "\n",
    "    x = Conv2D(filters=256, kernel_size=3, padding='same', activation='relu', use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dropout(DROPOUT_RATE)(x) \n",
    "\n",
    "    x = Conv2D(filters=256, kernel_size=3, padding='same', activation='relu', use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dropout(DROPOUT_RATE)(x) \n",
    "    x = UpSampling2D((4, 4), interpolation=\"bilinear\")(x)\n",
    "\n",
    "    \"\"\" Outputs \"\"\"\n",
    "    x = Conv2D(7, (1, 1), name='output_layer')(x)\n",
    "    x = Activation('softmax')(x)\n",
    "\n",
    "    \"\"\" Model \"\"\"\n",
    "    model = Model(inputs=inputs, outputs=x)\n",
    "    return model\n",
    "\n",
    "'''\n",
    "TRAIN\n",
    "'''\n",
    "def DeepLab_Train(model, X_train, y_train, X_val, y_val, weights_filename):\n",
    "    \n",
    "    try:\n",
    "        with tf.device(gpu_p40):\n",
    "            model.compile(\n",
    "                optimizer=Adam(learning_rate=LEARNING_RATE), \n",
    "                loss=LOSS_, \n",
    "                metrics=METRICS_)\n",
    "            \n",
    "            callbacks = [\n",
    "                ModelCheckpoint(weights_filename, save_best_only=True, save_weights_only=True, verbose=1),\n",
    "                EarlyStopping(patience=PATIENCE, verbose=1),\n",
    "                ReduceLROnPlateau(factor=FACTOR, patience=PATIENCE, min_lr=LEARNING_RATE, verbose=1)]\n",
    "            \n",
    "            history = model.fit(\n",
    "                X_train, y_train,\n",
    "                validation_data=(X_val, y_val),\n",
    "                batch_size=BATCH_SIZE, epochs=EPOCHS, callbacks=callbacks, verbose=2)\n",
    "            \n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "'''\n",
    "SCORE\n",
    "'''\n",
    "def DeepLab_Score(model, weights_filename, metrics_filename, X_test, y_test):\n",
    "    \n",
    "    try:\n",
    "        with tf.device(gpu_p40):\n",
    "            score = model.evaluate(X_test, y_test, batch_size=2, verbose=1)\n",
    "\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "    # Print and save the test metrics\n",
    "    print(\"Test loss:\", score[0])\n",
    "    print(\"Test accuracy:\", score[1])\n",
    "\n",
    "    with open(metrics_filename, \"a\") as f:\n",
    "        f.write(f\"\\Test Loss: {score[0]}\\n\")\n",
    "        f.write(f\"\\Test Accuracy: {score[1]}\\n\")\n",
    "        \n",
    "'''\n",
    "PREDICT\n",
    "'''\n",
    "def DeepLab_Predict(model, X_rgb_test):\n",
    "    try:\n",
    "        with tf.device(gpu_p40):\n",
    "            pred = model.predict([X_rgb_test], batch_size=BATCH_SIZE)\n",
    "            \n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "        \n",
    "    return pred\n",
    "\n",
    "'''\n",
    "DISPLAY RANDOM RESULT\n",
    "'''\n",
    "def DeepLab_Display(X_rgb_test, y_test, y_pred):\n",
    "\n",
    "    try:\n",
    "        with tf.device(gpu_p40):\n",
    "            fig, axes = plt.subplots(1, 3, figsize=(10, 6))\n",
    "            n = random.randint(0, len(X_rgb_test)-1)\n",
    "            cmap = plt.get_cmap('tab10', 7)\n",
    "    \n",
    "            axes[0].imshow(X_rgb_test[n])\n",
    "            axes[1].imshow(np.argmax(y_test[n], axis=-1), cmap=cmap)\n",
    "            axes[2].imshow(np.argmax(y_pred[n], axis=-1), cmap=cmap)\n",
    "                                           \n",
    "            axes[0].set_title(\"RGB\")\n",
    "            axes[1].set_title(\"Annotation\")\n",
    "            axes[2].set_title(\"Predicted\")\n",
    "    \n",
    "            for ax in axes.flatten():\n",
    "                ax.axis(\"off\")\n",
    "        \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "'''\n",
    "METRICS\n",
    "'''\n",
    "def DeepLab_Metrics(y_test, y_pred, metrics_filename):\n",
    "\n",
    "    try:\n",
    "        with tf.device(gpu_p40):\n",
    "            y_pred_classes = np.argmax(y_pred, axis=-1)\n",
    "            y_true_classes = np.argmax(y_test, axis=-1)\n",
    "    \n",
    "            num_classes=7\n",
    "\n",
    "            iou_values, mean_iou = iou(y_true_classes, y_pred_classes, num_classes)\n",
    "            pixel_acc = pixel_accuracy(y_true_classes, y_pred_classes)\n",
    "            mean_acc = mean_accuracy(y_true_classes, y_pred_classes, num_classes)\n",
    "            fw_iou_value = fw_iou(y_true_classes, y_pred_classes, num_classes)\n",
    "\n",
    "            print(f\"Mean IoU: {mean_iou}\")\n",
    "            print(f\"Pixel accuracy: {pixel_acc}\")\n",
    "            print(f\"Mean accuracy: {mean_acc}\")\n",
    "            print(f\"Frequency-Weighted IoU: {fw_iou_value}\")\n",
    "    \n",
    "            with open(metrics_filename, \"a\") as f:\n",
    "                f.write(\"\\nIoU Values:\\n\")\n",
    "                for i, iou_val in enumerate(iou_values):\n",
    "                    f.write(f\"Class {i}: {iou_val}\\n\")\n",
    "                f.write(f\"\\nMean IoU: {mean_iou}\\n\")\n",
    "                f.write(f\"Pixel Accuracy: {pixel_acc}\\n\")\n",
    "                f.write(f\"Mean Accuracy: {mean_acc}\\n\")\n",
    "                f.write(f\"Frequency Weighted IoU: {fw_iou_value}\\n\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2326e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "FILENAMES\n",
    "'''\n",
    "exp1_weights_filename = os.path.join(WEIGHTS_PATH, 'deeplab_baseline_exp1.h5')\n",
    "exp1_metrics_filename = os.path.join(METRICS_PATH, 'deeplab_baseline_exp1.txt')\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9901ae45",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "model\n",
    "'''\n",
    "try:\n",
    "    with tf.device(gpu_p40):\n",
    "        exp1_model = DeepLab(INPUT_DIMS)         \n",
    "except RuntimeError as e:\n",
    "    print(e)\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3fcde5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "TRAIN\n",
    "'''\n",
    "DeepLab_Train(\n",
    "    exp1_model, exp1_rgb_X_train, exp1_y_train, exp1_rgb_X_val, exp1_y_val, exp1_weights_filename)\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08f9137",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "SCORE\n",
    "'''\n",
    "DeepLab_Score(\n",
    "    exp1_model, exp1_weights_filename, exp1_metrics_filename, exp1_rgb_X_test, exp1_y_test)\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ec07cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "PREDICT\n",
    "'''\n",
    "exp1_y_pred = DeepLab_Predict(exp1_model, exp1_rgb_X_test)\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c22a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "DISPLAY RANDOM RESULT\n",
    "'''\n",
    "DeepLab_Display(exp1_rgb_X_test, exp1_y_test, exp1_y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45cea533",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "METRICS\n",
    "'''\n",
    "DeepLab_Metrics(exp1_y_test, exp1_y_pred, exp1_metrics_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab59105a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
